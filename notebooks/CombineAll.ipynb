{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "current_folder = os.path.abspath(os.curdir)\n",
    "root_folder = os.path.dirname(current_folder) \n",
    "print(f'importing root folder as \"{root_folder}\"...')\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(1, root_folder)\n",
    "from src.process.trainer import convert_to_sklearn_format, calc_doc_score\n",
    "from src.utils.logger import get_logger, log_params, get_log_level\n",
    "logger = get_logger('notebook.log', no_stdout=False, set_level='info')\n",
    "\n",
    "from src.process.cleaner import clean_data\n",
    "from src.utils.files import set_seeds, open_json\n",
    "\n",
    "set_seeds(1234)\n",
    "data_folder = 'D:/61 Challenges/2020_AESPEN_ESCI/EventSentenceCoref/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = os.path.join(data_folder, 'train.json')\n",
    "# _df = open_json(file_name, data_format=list)\n",
    "# logger.info('-- loaded files')\n",
    "# _df = clean_data(_df, False, None,'en')\n",
    "# logger.info('-- cleaned files')\n",
    "# all_df.extend(_df)\n",
    "# logger.info(f'-- number of docs: {len(all_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_folder, 'en-train.json')\n",
    "_df = open_json(file_name, data_format=list)\n",
    "logger.info('-- loaded files')\n",
    "_df = clean_data(_df, False, None,'en')\n",
    "logger.info('-- cleaned files')\n",
    "all_df.extend(_df)\n",
    "\n",
    "pair_count = 0\n",
    "sent_count = 0\n",
    "for i in _df:\n",
    "    s = len(i['sentence_no'])\n",
    "    sent_count += s\n",
    "    pair_count += (s*(s-1))/2\n",
    "logger.info(f'-- number of docs: {len(_df)}')\n",
    "logger.info(f'-- number of sentences: {sent_count}')\n",
    "logger.info(f'-- number of pairs: {pair_count}')\n",
    "logger.info(f'-- TOTAL number of docs: {len(all_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_folder, 'es-train.json')\n",
    "_df = open_json(file_name, data_format=list)\n",
    "logger.info('-- loaded files')\n",
    "_df = clean_data(_df, False,None,'es')\n",
    "logger.info('-- cleaned files')\n",
    "all_df.extend(_df)\n",
    "\n",
    "pair_count = 0\n",
    "sent_count = 0\n",
    "for i in _df:\n",
    "    s = len(i['sentence_no'])\n",
    "    sent_count += s\n",
    "    pair_count += (s*(s-1))/2\n",
    "logger.info(f'-- number of docs: {len(_df)}')\n",
    "logger.info(f'-- number of sentences: {sent_count}')\n",
    "logger.info(f'-- number of pairs: {pair_count}')\n",
    "logger.info(f'-- TOTAL number of docs: {len(all_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_folder, 'pr-train.json')\n",
    "_df = open_json(file_name, data_format=list)\n",
    "logger.info('-- loaded files')\n",
    "_df = clean_data(_df, False,None,'pt')\n",
    "logger.info('-- cleaned files')\n",
    "all_df.extend(_df)\n",
    "\n",
    "pair_count = 0\n",
    "sent_count = 0\n",
    "for i in _df:\n",
    "    s = len(i['sentence_no'])\n",
    "    sent_count += s\n",
    "    pair_count += (s*(s-1))/2\n",
    "logger.info(f'-- number of docs: {len(_df)}')\n",
    "logger.info(f'-- number of sentences: {sent_count}')\n",
    "logger.info(f'-- number of pairs: {pair_count}')\n",
    "logger.info(f'-- TOTAL number of docs: {len(all_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "out=(os.path.join(data_folder, 'all_train.json'))\n",
    "with open(out, \"w\") as outfile:\n",
    "     for obj in all_df:\n",
    "        outfile.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(data_folder, 'all_train.json')\n",
    "_df = open_json(file_name, data_format=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-buffer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
